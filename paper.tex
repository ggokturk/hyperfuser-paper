\documentclass[10pt,journal,compsoc]{IEEEtran}
\usepackage{subfiles}
\usepackage[pdftex]{graphicx}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{subfig}
\usepackage[noend]{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:~}}
\renewcommand{\algorithmicensure}{\textbf{Output:~}}
\newcommand\ovr[1]{\overrightarrow{#1}}

\usepackage{array}
\usepackage{color}
\usepackage{listings}
\usepackage{placeins}
\usepackage{enumitem}


\ifCLASSOPTIONcompsoc
  \usepackage[nocompress]{cite}
\else
  \usepackage{cite}
\fi

\ifCLASSINFOpdf
\else
\fi

\newcommand\MYhyperrefoptions{bookmarks=true,bookmarksnumbered=true,
pdfpagemode={UseOutlines},plainpages=false,pdfpagelabels=true,
colorlinks=true,linkcolor={black},citecolor={black},urlcolor={black},
pdftitle={Bare Demo of IEEEtran.cls for Computer Society Journals},%<!CHANGE!
pdfsubject={Typesetting},%<!CHANGE!
pdfauthor={Michael D. Shell},%<!CHANGE!
pdfkeywords={Computer Society, IEEEtran, journal, LaTeX, paper,
             template}}%<^!CHANGE!


\begin{document}

\title{Fast Influence Maximization Sketching\\ with Fusing and Vectorization }


\author{G\"{o}khan~G\"{o}kt\"{u}rk
        and~Kamer~Kaya% <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem G. G\"{o}kt\"{u}rk and K. Kaya are with Computer Science and Engineering, Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}
}
%FIXME



\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Advanced Demo of IEEEtran.cls for IEEE Computer Society Journals}


\IEEEtitleabstractindextext{%
\begin{abstract}
The abstract goes here.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Computer Society, IEEE, IEEEtran, journal, \LaTeX, paper, template.
\end{IEEEkeywords}}


\maketitle

\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle


\ifCLASSOPTIONcompsoc
\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
\else
\section{Introduction}
\label{sec:introduction}
\fi

Influence maximization is the problem of finding subset of $K$ vertices in a graph $G$ that has maximum reachability on some diffusion model $M$. 

In addition to introducing the problem, Kempe et al.\cite{kempe2003maximizing} proved it to be NP-hard and provided a greedy Monte-Carlo approach that has a constant approximation for the optimal solution.




Many heuristics and proxy methods have been proposed in the literature~\cite{MixGreedy, narayanam2010shapley, kimura2007extracting, chen2010PMIA,chen2010LDAG, kim2013scalable, skim, goyal2011simpath, jung2012irie,cheng2014imrank,liu2014influence,galhotra2016holistic}.


\section{Notation and Background}\label{sec:background}

We denoted $G = (V,E)$ as a directed graph where the $n$ vertices in $V$ represent the agents, and $m$ edges in $E$ represent the relations between the agents in $V$.
The neighborhood of a vertex $u \in V$ is denoted as $\Gamma_G(v) = \{v: (u,v) \in E\}$. 
A graph $G' = (V',E')$ is a sub-graph of $G$ if $V' \subseteq V$ and $E' \subseteq E$. 
Diffusion probability on edge $(u, v) \in G$ is noted as $w_{u,v}$.
$w_{u,v}$ can be determined either by the diffusion model or according to the strength of $u$ and $v$'s relationship in the data.

\begin{table}[!ht]
    \caption{Table of notations}
    \label{tab:notation}
    \centering
    \begin{tabular}{|l|p{0.7\linewidth}|}
        \hline
        Variable & Definition  \\
        \hline
        $G = (V,E)$     & Graph $G$ with vertices $V$ and edges $E$ \\
        $\Gamma_G(v)$   & Neighborhood of vertex $v$ in graph $G$\\
        $w_{u,v}$       & Probability of $u$ directly influencing $v$ \\
        %$SCC(v) $       & Strongly connected component of vertex $v$\\
        $R_{G}(v)$      & Reachability set of vertex $v$ on graph $G$\\
%        $\overline{R_{G}(v)} & Complement of the vertex set $R_{G}(v)$\\
        \hline\hline
        $S$             & Seed set to maximize influence\\
        $K$             & Size of the seed set\\
        $\mathcal{R}$   & Number of Monte-Carlo simulations performed\\
        $\sigma_{G}(S)$ & Influence score of $S$ in $G$, i.e., expected number of vertices reached from $S$ in $G$\\
        % $\sigma_{G}{(S,v)}$          & Marginal influence gain by adding vertex $v$ to seed set $S$\\
\hline\hline
        %$p$             & Sampling probability for edges\\
%        $P(s,v)_r $     & Random probability generated for selecting edge vertices $s$ to $v$ in simulation $r$\\
        $X_{r..n}$      & Slice of Vector X between r and r+n(not including)\\
        $h(u,v)$        & Hash function for edge $\{u,v\}$\\
        $h_{max}$       & Maximum value hash function $h$ can return\\
        %$X_r$           & Random number/hash generated for simulation $r$  \\
        \hline\hline
        $B$             & Batch size\\
        $e$             & Estimated reachability set size\\
        % $[a, \ldots, a]_B$      & Vector of size $B$, contains all $a$\\
        $M_u[j]$        & Sketch $j$th register for vertex $u$\\
        $\varsigma $    & Influence gained before sketch build\\
        $\sigma $       & Influence Score\\
        $\delta$        & Marginal Gain after sketch build\\
        $\epsilon_g$    & Allowed global estimation error ratio\\
        $\epsilon_l$    & Allowed local estimation error\\ 
        $\epsilon_c$    & Allowed non-convergenced vertex ratio\\
      %  $x[[i, j]]$   & Slice of vector  $x$ between indices $i$ and $j$, not including $j$ \\ 
        \hline         
    \end{tabular}
\end{table}
\subsection{Influence Maximization}

Influence Maximization aims to find a seed set $S \subseteq V$ among all possible size $K$ subsets of $V$ that maximizes an {\em influence spread function} $\sigma$  when the diffusion process is initiated from $S$. Although we focus on undirected graphs, for IM, the graph can be directed or undirected depending on the initial construction. Figure~\ref{fig:xx} shows a~(Fig.~\ref{fig:ic}) and directed~(Fig.~\ref{fig:wc})
graph for which the weights on the edges are diffusion/influence probabilities. 

\begin{figure}[!ht] 
    \centering
  \subfloat[\small{IC}\label{fig:ic}]{%
       \includegraphics[width=0.45\linewidth]{./images/ic-dif.png}}
  \subfloat[\small{WC}\label{fig:wc}]{%
        \includegraphics[width=0.45\linewidth]{./images/wc-dif.png}}
    \\% IMAGES HERE
  \caption{\protect\subref{fig:ic} 
The directed graph $G = (V, E)$ for Independent Cascade with independent diffusion probabilities. 
\protect\subref{fig:wc}
The directed graph obtained from the directed one by setting the diffusion probabilities of incoming edges to $1 / |\Gamma_G(v)|$ for each vertex $v \in V$. 
  }
  %\label{fig_ic_model} 
  \label{fig:xx} 
\end{figure}
The influence spread function $\sigma_{G,M}(\cdot)$ computes the {\em expected} number of agents/nodes/vertices influenced~(activated) through a diffusion model $M$. For the sake of simplicity, we drop $M$ from the notation; in the rest of the text, $\sigma_{G}$ refers to $\sigma_{G,M}$. Some of the popular diffusion models for IM in the literature are {\em independent} and {\em weighted cascade}~(IC and WC),  and {\em linear threshold}~(LT)~\cite{kempe2003maximizing}. 


\subsection{Sketch}\label{sec:sketch}
The count-distinct problem is the problem of finding the number of distinct elements in a stream with non-distinct elements. Computing reachability sets of a vertex is a similar problem; cardinality of all vertices visited is calculated while traversing sample sub-graphs from the given vertex. Exact cardinality calculations requires memory proportional to the cardinality. 

Reachability set of a vertex is distinct union of all connected vertices' reachability sets. Many other Influence Maximization methods exploit this property in some degree. 

The methods based on Reverse reachability and Bottom-Up Traversal, utilizes this property directly to merge reachability sets of connected vertices to estimate vertices influence.
MixGreedy method goes one step further; it utilizes that all vertices in a connected component that have the same reachability set in undirected graphs. So that, for a sample graph, all reachability sets can be found in single graph traversal. 

For directed graphs, storing reachability sets for all vertices and merging these sets are infeasible for non-trivial graphs. 
If one-hot vectors are used to store reachability sets for constant insertion time, $O(n^2\mathcal{R})$ bits of memory is required and each merge operation has $O(n)$ time complexity. 
if Disjoints sets are used for storing reachability sets; $(n\bar{\sigma}\mathcal{R})$ memory is required to store all reachability sets, and each merge operation has $O(ackermann'(\sigma))$ complexity.

Count-distinct sketches have nice properties for such problems; Flajolet–Martin algorithm\cite{flajolet1985probabilistic} can estimate cardinality of distinct elements in reachability sets with constant number of registers($M$), and most importantly union of these sets can be done in constant number of operations.

Flajolet–Martin sketch only stores how rare elements are in a multi-set.Rarity of the elements are often computed by counting leading zeros in item's hash values.  The registers are initialized with zero values in begining. Items are added by storing longest leading zero count. Cardinality estimation can be done by taking 2 to the power of the stored value. Multiple registers and hash functions, $M[j]$ and $h_j$, are commonly used to reduce variance.
Adding item $x$ to register $M$, and its result $M'$ is shown in equation~\ref{eq:sketch-add}.

\begin{equation}
    \label{eq:sketch-add}
    M'[j] = max(M[j],clz(h_j(x)) ~ \forall j\in(0,J]
\end{equation} where $clz$ is count leading zeros function and $J$ is number of sketch registers.

The merge operation for two sketches, $M_u$,$M_v$, can be performed by computing the pairwise maximum of the registers. Formally; 
\begin{equation}
    \label{eq:sketch-merge}
    M'[j] = max(M_u[j],M_v[j]) ~ \forall j\in(0,J]
\end{equation} 
where $M'$ is union of set and $J$ is number of sketch registers.

Average of longest running zeros can be used compute the cardinality and the result is divided to correction factor $\phi \approx 0.77351$ to correct hash collisions.
\begin{equation}
    \label{eq:sketch-estimate}
    e = 2^{\bar{M}}/\phi
\end{equation} 
where $e$ distinct cardinality and $\bar{M}$ is M's registers mean.

In this work, we utilize a variant of Flajolet–Martin sketch; since multiple Monte-Carlo simulations are performed to calculate estimated influence, we use one register per simulation, and take average of longest leading zero counts to calculate average cardinality of reachability sets. Also, in an another perspective, it is same as using register as much as MC simulation, where updates are done with same probability as influence propagation. 


\subsection{SIMD}
{\em Single Instruction-Multiple Data} machines exploits data level parallelism to perform the same operation on multiple data points simultaneously. In this work, we utilized Advanced Vector Extensions (AVX2) instruction set. AVX2 works on 256-bit registers in multiple packed forms; 8x32 bit operations are used in fused sampling, where as sketch merge  performed with 32x8bit operations. This dichotomy between operations solved by processing in 32 simulations batches at a time.
Even-though, auto-vectorization is enabled, most of the operation mention are cannot be vectorized by the compiler. As far as we know, no compiler vectorizes chain conditional move or move-mask operations. Vectorization in the implementation is done using explicit intrinsics.

% First, active edges are checked by generating random values, $P(u,v)_r$. After comparing random values with edge threshold, active edges are store in an 32bit integer as one-hot vector. Then,  


For completeness, the intrinsics explicitly used in this paper are described in Table~\ref{tab:avx2-instructions}.

\begin{table}[!ht]
    \caption{AVX2 intrinsics used in the implementation.}
    \label{tab:avx2-instructions}
    \centering

    \begin{tabular}{|p{0.33\linewidth}|p{0.58\linewidth}|}
    \hline
    Intrinsic & Definition\\
    \hline
    {\tt \_mm256\_set1\_epi32} & Initializes 256-bit vector with scalar integer values. Doesn't map to any AVX instructions. \\
        % \hline
    {\tt \_mm256\_and\_si256} & Performs bitwise logical AND operation on 256-bit integer vectors. \\
            % \hline
    {\tt \_mm256\_xor\_si256} &  Performs bitwise logical XOR operation on 256-bit integer vectors.\\
            % \hline
    {\tt \_mm256\_cmpgt\_epi32} & Compares packed 8x 32-bit integers of two input vectors. \\
            % \hline
        % \hline
    {\tt \_mm256\_movemask\_ps} & Extracts the first bits of 8x 32-bit elements in a compact 8-bit format \\
            % \hline
    {\tt \_mm256\_blendv\_epi8} & Blends/selects byte elements of input vectors depending on the bits in a given mask vector.\\ 
             \hline         
    \end{tabular}
\end{table}{}

\section{Method}\label{sec:method}

\subsection{Hash-based Fused Sampling}
The probabilistic nature of cascade models requires sampling sub-graphs $\hat{G}$ from $G = (V, E)$ to simulate the diffusion process.  Sampling can be a dominating factor in processing; sampling may demand multiple passes on the graph, if samples are memoized, multiple times of the graph size needs to be stored.
In this work, samples are not generated explicitly. By using a hash-based, fused sampling, we eliminate the necessity of in-memory creation and storage of the sample sub-graphs.
While processing an edge, it is considered for all possible samples.
It is sampled or skipped depending on the outcome of hash based random value. The hash function used is given in equation \ref{eq:hash}
\begin{equation}
    \label{eq:hash}
    h(u,v) = \mbox{{\sc Murmur3}}(u||v)  
\end{equation}
where $||$ is the concatenation operator. 

The {{\sc Murmur3}}~\cite{MurmurHash3} hash function is preferred because of its simplicity, good avalanche behavior and bias.

For performance considerations, edge hash values were pre-computed in our work and only existing pairs were considered. The trade-off between extra memory and computation here maybe not be applicable for different architectures and faster/simpler hash functions.

Although the above-mentioned approach generates a unique hash value for each edge, and hence a unique sampling probability, different simulations require different probabilities.

Multiple deterministic random values for each edge is generated by using the edge hash values and random numbers $X_r$ associated with each simulation $r$. 
Sampling probability of $(u, v)$ for simulation $r$, $P(u, v)_r$, is computed as follows; First, $h(u,v)$ is XOR'ed with a uniformly randomly chosen $X_r \in_R [0, h_{max}]$ and the result is normalized by dividing the value to the upper limit of the hash value $h_{max}$. Formally,
\begin{equation}
    \label{eq:hash_prob}
    {\rho}(u,v)_r = \frac{X_r \oplus h(u,v)}{h_{max}}.
\end{equation}

The edge $\{u,v\}$ is verified to be in the sample if ${\rho}(u,v)_r$ is smaller than or equal to the threshold $w_{u,v}$. 
The edge $\{u,v\}$ is exists in the sample $r$ if and only if  ${P}(u,v)_r$ is smaller than the edge threshold $w_{u,v}$. One of the benefits of this approach is, sampling an edge is only single XOR and compare-greater-than operations. In addition, the following control flow branch can be removed using conditional move operations. An efficient SIMD implementation of this approach will be discussed later in this section.

Using a strong hash function such as {{\sc Murmur3}} ensures all bits independently change if the input is changed. This property allows us to generate good enough pseudo-random values for fair sampling. To prove the fairness of random values generated with the hash-based approach, we generated a large number of samples for various real-life networks and graphed the Cumulative Distribution Function~(CDF) of random values $P(u,v)_r$ used while sampling.

For a given graph $G = (V,E)$, the CDF of a sampling probability $x$ is computed as $\Pr\left(x \leq P(u,v)_r\right)$ for all $(u,v) \in E$ and $0 \leq r < R$. Figure~\ref{fig:prob_cdf} shows the CDFs for 12 real-life networks. The sampling probability distribution with hash-based computation is almost identical with the uniform distribution which is used to simulate the influence diffusion process.

\begin{figure}[!ht] 
    \centering
    \includegraphics[width=1\linewidth]{./images/cdf.pdf}
    \caption{Cumulative distribution function of hash-based sampling probabilities on various real-life networks.}
    \label{fig:prob_cdf} 
\end{figure}

%Traditional implementations sample each edge in $G$ once per simulation and store them to construct a sampled subgraph. n
Being able to generate infinitely many samples on-the-fly, allows us to avoid many memory accesses. The only down-side of hash-based fused sampling is that we have to generate all random values,   $P(u,v)_r$, for each edge traversal.  Fortunately, these computations are quite fast on modern computing hardware.

\subsection{Reachability Set Cardinality Estimation}
A greedy solution to the influence maximization problem requires finding a vertex that maximizes marginal influence gain at each step until the seed set size reaches $K$. And, Single vertex influence estimation is the same problem as the count-distinct problem applied to all sample sub-graphs. So, it is possible to use Count-distinct sketches to estimate influence scores in the greedy method. 

We propose an influence maximization method, that utilizes the Count-distinct sketch we mentioned in section \ref{sec:sketch} to estimate averages of distinct elements in sample subgraphs for both time and space efficiency. Algorithm \ref{algo:main} describes the method proposed as follows; 

First, reachability sets of all vertices are initialized with vertices themselves. For all vertices $u$, its $j$th register is set to $M_u[j]=clz(h_j(u))$ meaning $R_{\hat{G}_j}(u) = \{u\}$. Then, we perform diffusion process on the sketch registers. 

The diffusion process works as follows; first, all vertices added to live vertices set $L$. Then, incoming edges $(u,v)$ for all live vertices are processed. For all simulation $u$ is live, the sketch of both sides are merged and set to $u$ registers. If any of the sketch-registers change, vertex $u$ added to the next iteration's live vertices set $L'$. After live edges are processed, if the next iteration's live vertices set is smaller than the convergence threshold $\epsilon_c$, the diffusion process concluded. If not, we swap $L$ and $L'$, clear $L'$, and then diffusion processed is repeated. 

In Greedy algorithm\cite{kempe2003maximizing} for each candidate vertex, influence diffusion computations are perform for each vertex.  On the other hand, diffusion on sketch registers allows finding all vertices reachability set. All vertices influence in single step distance only computed once. This dynamic programming like property allows fast selection of influence maximizing candidate.


After diffusion, following process is repeated until $K$ vertices are added to the seed set; For all vertex $u$ reachability set cardinalities, $R_G(S\cup u)$, estimated by merging $mask$ and their sketch registers. The $mask$ used for the sketch of $R_G(S)$ and initialized with zeros. Vertex with maximum cardinality is selected to add to the seed set. Then, the actual Monte-Carlo simulations were performed to compute the reachability set of the seed set. Having actual $R_G(S)$ allows both to calculate estimation errors and to find blocked vertices for all simulations. Blocked vertices skipped in the diffusion step in coming iterations. Besides, the real influence score allows us to rebuild our sketch when the sketches have gone stale.

The sketches are assumed as stale if the absolute error in estimated total influence score is more than global threshold, $\epsilon_{g}$, and the marginal gain estimation error after the last sketch larger than local threshold $\epsilon_{l}$. Two threshold values allow the algorithm to rebuild after large local errors but skip rebuilding if the error is minimal in the total score. If a rebuild is not required, only the last added vertex's sketch also merged with $mask$ to update $R_G(S)$. 




\begin{algorithm}
\caption{\sc{SketchFuser}($G,K,J$)}
\label{algo:main}
\algorithmicrequire{$G = (V,E)$: the influence graph
\\\hspace*{6.6ex}{$K$: number of seed vertices
\\\hspace*{6.7ex}$\mathcal{J}$: number of MC simulations}\\}
\algorithmicensure{$S$: a seed set that maximizes influence on $G$
}
\begin{algorithmic}[1]
    \State {$S \leftarrow \{\emptyset\}$}
    \For{$ v\in V$}
        \For{$ j\in J$}
            \State $M[j]_v \leftarrow clz(hash(v) \oplus hash(j))$ 
        \EndFor
    \EndFor
    \State $M \leftarrow Simulate(G,M,\emptyset)$
    \State $mask \leftarrow zeros(J)$
    \State $\varsigma \leftarrow 0$
    \For{$k=1\ldots K$}
        \State $s \leftarrow \underset{v\in V}{\mathrm{argmax}} ~estimate(merge(mask,M_v))$
        \State $S \leftarrow S \cup \{s\}$
        \State $e \leftarrow estimate(merge(mask,M_s))$
        % \State $R_S \leftarrow run\_cascade(G,S,J)$
        \State {Compute seed set $S$'s Reachability set $R_S(S)$ }
        \State $\sigma_G(S) \leftarrow |R_S|/J$
        \State $\delta = \sigma - \varsigma$
        \If{$ (e - \delta) / \delta < \epsilon_l \lor |e-\delta| / \sigma < \epsilon_g$}
            \State $mask \leftarrow merge(mask,M_s)$
        \Else
            \For{$ v\in V$}
                \For{$ j\in J$}
                    \State $M[j]_v \leftarrow clz(hash(v) \oplus hash(j))$ 
                \EndFor
            \EndFor
            \State $M \leftarrow Simulate(G,M,R_S)$
            \State $mask \leftarrow zeros(J) $ 
            \State $\varsigma \leftarrow \sigma $ 
        \EndIf
    \EndFor
    \State \Return $S$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
\caption{\sc{Simulate}($G,M,J,R_S$)}
\label{algo:diffusion-step}
\algorithmicrequire{$G = (V,E)$: the influence graph
\\\hspace*{6.6ex}{$M$: Sketch vectors of vertices
\\\hspace*{6.7ex}$\mathcal{J}$: number of MC simulations
\\\hspace*{6.7ex}$R_S$: Reachability set of the seed set
}
\\}
\algorithmicensure{$M$: Updated Sketch vectors
}
\begin{algorithmic}[1]
    \State {$L \leftarrow V$}
    \State {$L' \leftarrow {\emptyset}$}
    \While{$|L|/|V| > \epsilon_c$}
        \For{$e_{u,v} \in \Gamma(L)$} %REVERSE THIS
            \For{$j \in (0,J]$}
                \If{$P(u,v)_j < W_{u,v} \land u  \not\in R_S[j]$}
                    \State{$M_u[j]\leftarrow max(M_u[j],M_v[j])$}                
                \EndIf
            \EndFor
            \If {$M_u$ changed}
                \State $L' \cup u $
            \EndIf
        \EndFor
        \State $L \leftarrow L'$
        \State $L' \leftarrow \{\emptyset\}$
    \EndWhile
\end{algorithmic}
\end{algorithm}
\FloatBarrier
\subsection{Vectorization and Performance considerations}

Traditional two-step, sample-then-diffuse computation model stores data loosely-coupled. Sampled edges are stored together, diffusion related information stored afar. This way, SIMD operation is allowed on consecutively located edges, requiring random memory accesses for the diffusion process. Even if tightly coupled
 
One of the main advantages of fused sampling is allowing similar simulation data and computations together. 
Our implementation generates $B/4$ sample edges and runs $B$ simulations at a time. 
The dichotomy of two batch sizes comes from SIMD width; 
sampling probabilities are computed in 31-bit fixed-point precision, 
whereas since we are using 64-bit hash values, sketch registers only require $\left\lceil log2(64+1) \right\rceil = 6$ bits. Aligned to the nearest data type they can fit, sampling probabilities utilizes 8x32-bit operators, where as sketch operations employ 32x8bit SIMD vectors.


% \subsubsection{Vectorized edge processing} first loads edge hash and threshold to vector units. Then, we check if the edge is live in any simulations. 8 random values generated by XOR'ing hash vector and X_r


\begin{algorithm}
    \caption{\sc{Vectorized-Merge} ({$u,v,X_r,M_u,M_v,$})}\label{alg:VecLabel}
    \algorithmicrequire{
    $u:$ source vertex\\
    \hspace*{7ex}$v:$ target vertex\\
    \hspace*{7ex}$X$: random number for simulations\\
    \hspace*{7ex}$M_u$: sketch registers of $u$\\
    \hspace*{7ex}$M_v$: sketch registers of $v$\\
    \hspace*{7ex}$visited$: one-hot visited vector grouped by samples}\\
    \algorithmicensure{$islive$: a boolean which state if $u$ is live}
    % \hspace*{8ex}$live_v$: a boolean which state if $v$ is live
    \begin{algorithmic}[1]
        \State $islive \leftarrow False$
        \For{$r \in [0,J)~|~r~mod~32 = 0$}
            \State $hashvec \leftarrow \_mm256\_set1\_epi32(hash(u,v)$
            \State $threshold \leftarrow \_mm256\_set1\_epi32(w_{u,v})$
            \State $live \leftarrow 0$
            \For{$b\in [0,32)~|~b~mod~8 = 0 $}
                \State $prob \leftarrow \_mm256\_xor\_si256(X_{r+b\dots8},hashvec)$
                \State $cmp \leftarrow \_mm256\_cmpgt\_epi32(threshold, prob)$
                \State $cmpmask \leftarrow \_mm256\_movemask\_ps(cmp))$
                \State $live\leftarrow (cmpmask<<b)~|~live $
                \EndFor
                \If{$live = 0$}
                    \State continue
                \EndIf
                \State $live \leftarrow live~\&~\sim visited_{r\dots32}[u]$
                \State $livebytes \leftarrow expand\_bits\_to\_bytes(live)$
                \State $M' \leftarrow 
                    \_mm256\_cmpgt\_epi8(M_u[r..32],M_v[r..32])$
                \State $select \leftarrow \_mm256\_and\_si256(livebytes,M')$
                \State $\_mm256\_blendv\_epi8(M_u[r..32],M_v[r..32])$
                % \State $$
                \State $islive \leftarrow islive~|~  \_mm256\_cmp\_ps(\newline
                    \hspace*{4em}\_mm256\_setzero\_ps(), \newline
                    \hspace*{4em}\_mm256\_castsi256\_ps(sel))$


                % \State $live \leftarrow live \setminus \bigcup_{} j|u\notin R_{\hat{G}_j} $ 
            % \EndIf
        \EndFor
    \end{algorithmic}
    \end{algorithm}

\section{Evaluation}\label{sec:evaluation}

\section{Related Work}\label{sec:relatedwork}

\section{Conclusion}\label{sec:conclusion}

\ifCLASSOPTIONcaptionsoff
  \newpage
\fi


\bibliographystyle{IEEEtran}
\bibliography{refs}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./images/gokhan.png}}]{G\"{o}khan~G\"{o}kt\"{u}rk} is a PhD candidate at the Faculty of Engineering and Natural Sciences in Sabancı University. He has received his BS and MS degrees from Sabancı University as well. He is interested in High Performance Computing, Parallel Programming, and Graph Processing.
\end{IEEEbiography}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./images/kamer.jpg}}]{Kamer Kaya} is an Assistant Professor at the Faculty of Engineering and Natural Sciences at Sabancı University. He got his PhD from Dept. Computer Science and Engineering from Bilkent University. He worked at CERFACS, France, as a post-graduate researcher in the Parallel Algorithms Project. He then joined the Ohio State University in September 2011 as a postdoctoral researcher, and in December 2013, he became a Research Assistant Professor in the Dept. of Biomedical Informatics.
His current research interests include Parallel Programming, High Performance Computing, and Cryptography. 
    \end{IEEEbiography}

\end{document}