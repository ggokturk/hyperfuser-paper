\documentclass[10pt,journal,compsoc]{IEEEtran}
\usepackage{subfiles}
\usepackage{url}
\usepackage[pdftex]{graphicx}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{subfig}
\usepackage[noend]{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:~}}
\renewcommand{\algorithmicensure}{\textbf{Output:~}}
\newcommand\ovr[1]{\overrightarrow{#1}}
\newcommand\myeq{\mkern1.5mu{=}\mkern1.5mu}
\usepackage{booktabs}
\usepackage{array}
\usepackage{color}
\usepackage{listings}
\usepackage{placeins}
\usepackage{enumitem}
%%%%%%
% notation 
%%%%%
\ifCLASSOPTIONcompsoc
  \usepackage[nocompress]{cite}
\else
  \usepackage{cite}
\fi

\ifCLASSINFOpdf
\else
\fi
\newcommand\ggx[1]{\textcolor{blue}{#1}}

\newcommand\MYhyperrefoptions{bookmarks=true,bookmarksnumbered=true,
pdfpagemode={UseOutlines},plainpages=false,pdfpagelabels=true,
colorlinks=true,linkcolor={black},citecolor={black},urlcolor={black},
pdftitle={Bare Demo of IEEEtran.cls for Computer Society Journals},%<!CHANGE!
pdfsubject={Typesetting},%<!CHANGE!
pdfauthor={Gokhan Gokturk, Kamer Kaya},%<!CHANGE!
pdfkeywords={Influence Maximization, Graph Processing, Graph Sampling, Fused Sampling, Memory Access Regularization, Count-Distinct Sketch}}%<^!CHANGE!

\newcommand\acro{{\sc{HyperFuseR\xspace}\xspace}\xspace}
\usepackage{xcolor}
\newcommand\kktodo[1]{\textcolor{red}{#1}}



\newcommand\minspeedup{{{3.5\xspace}\xspace}\xspace}
\newcommand\maxspeedup{{{11\xspace}\xspace}\xspace}

\newcommand\maxspeedupTIM{{{1500\xspace}\xspace}\xspace}
\newcommand\maxspeedupIMM{{{27.87\xspace}\xspace}\xspace}
\newcommand\maxspeedupSKIM{{{11\xspace}\xspace}\xspace}

\begin{document}

\title{Fast and Error-Adaptive Influence Maximization based on Count-Distinct Sketches}

\author{G\"{o}khan~G\"{o}kt\"{u}rk
        and~Kamer~Kaya% <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem G. G\"{o}kt\"{u}rk and K. Kaya are with Computer Science and Engineering, Faculty of Engineering and Natural Sciences, Sabanci University, TR~34956, Istanbul, Turkey.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}
}
%FIXME



% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
% {Shell \MakeLowercase{\textit{et al.}}: Bare Advanced Demo of IEEEtran.cls for IEEE Computer Society Journals}


\IEEEtitleabstractindextext{%
\begin{abstract}
Influence maximization~(IM) is the problem of finding a seed vertex set that maximizes the expected number of vertices influenced under a given diffusion model. Due to the NP-Hardness of finding an optimal seed set, approximation methods are frequently used for IM. In addition to these high-quality, yet expensive approximation algorithms, lightweight, sketch-based approaches, which do not simulate the process exactly, have been proposed in the literature to cope with the scale of today's networks. %As expected, the quality of the seed sets for the latter is inferior to that of the former. 
In this work, we describe a fast, error-adaptive approach that leverages Count-Distinct sketches and the recently proposed hash-based fused sampling. To estimate the number of influenced vertices throughout a diffusion, we use per-vertex Flajolet-Martin sketches where each sketch corresponds to a sampled subgraph. To efficiently simulate the diffusions, the reach-set cardinalities of a single vertex are stored in memory in a consecutive fashion. This allows the proposed algorithm to find the number of influenced vertices in a single simulation step of different simulations at once. In addition, thanks to their efficiency, and the scalability of our parallel implementation, we can rebuild the sketches whenever it is necessary. For a faster IM kernel, we rebuild the sketches only after observing estimation errors above a given threshold. Our experimental results show that the proposed method yields high-quality seed sets while being up-to $1.7\times$--$27.9\times$ faster on average than a state-of-the-art approximation algorithm. In addition, it is $3.5\times$--$11.1\times$ faster than a sketch-based approach while producing seed sets with $3\%$--$12\%$ better influence scores.
\end{abstract}
} 

% Note that keywords are not normally used for peer review papers.
% section 3, overview
% 
% 
% 
% 
% 
% 
% 
% 
% 

\maketitle

\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle


\ifCLASSOPTIONcompsoc
\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
\else
\section{Introduction}
\label{sec:introduction}
\fi

The study of efficient information/influence propagation in networks becomes an important area of research with several applications in many fields, such as viral marketing~\cite{leskovec2007dynamics, trusov2009effects}, social media analysis~\cite{zeng2010social, moreno2004dynamics}, and recommendation systems~\cite{lu2012recommender}.
As the study of these networks is imperative for educational, political, economic, and social purposes, a high-quality seed set to initiate the diffusion may have critical importance.
Furthermore, since the diffusion analysis may be time-critical, or increasing its coverage may be expensive in terms of computing resources and time spent, novel and efficient approaches to find good vertex sets that propagate the information/influence effectively are essential.

Influence maximization is the problem of finding a subset $S \subset V$ of $K$ vertices in a graph $G = (V, E)$ with the vertex set $V$ and edge set $E$ such that $S$ reaches the maximum reachability, i.e., influences the maximum number of vertices, under some diffusion model $M$. Kempe et al.~\cite{kempe2003maximizing} introduced the IM problem, proved it to be NP-hard, and provided a greedy Monte-Carlo approach that has a constant approximation ratio over the optimal solution. This greedy approach is one of the most frequently applied algorithms for IM. The time complexity of the greedy algorithm, estimating the $\sigma$ influence score, running $R$ simulations, and selecting $K$ seed vertices is $\mathcal{O}(KRn\sigma)$ for a graph with $n$ vertices. Although they perform well in terms of seed-set quality, the greedy Monte-Carlo solutions are impractical for real-life networks featuring millions of vertices as a consequence of their expensive simulation costs. Due to this reason, many heuristics and proxy methods have been proposed in the literature~\cite{MixGreedy, narayanam2010shapley, kimura2007extracting, chen2010PMIA,chen2010LDAG, kim2013scalable, cohen2014sketch, goyal2011simpath, jung2012irie,cheng2014imrank,liu2014influence,galhotra2016holistic}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
% However, these simulation-based, greedy algorithms provide the best possible approximation guarantees. Therefore they are considered as the gold standard for IM.  

Simulating a greedy algorithm in parallel is a straightforward workaround to reduce the execution time of IM kernels and make them scalable for large-scale networks. However, for large networks, a greedy approach with a good approximation guarantee does not come cheap even if a large number of processing units/cores are available on networks with billions of vertices and edges. Following similar attempts in the literature, we propose a parallel, sketch-based greedy approach that approximates the Monte-Carlo processes. The proposed approach does not exactly count the number of influenced vertices. Instead, it leverages Count-Distinct sketches. Below is a summary of our contributions:

\begin{itemize}
\item We propose \acro\footnote{\url{https://github.com/ggokturk/infuser}}, an open-source, blazing-fast, sketch-based and accurate Influence Maximization algorithm. The proposed scheme samples the edges as they are traversed across several simulations. Thus, sampling, diffusion, and count-distinct processes are fused for all simulations. 

\item Running concurrent simulations on per-vertex Count-Distinct sketches reduces the number of memory accesses. While traversing an edge, any number of diffusion simulations can be performed between vertices, using only a single (8-bit) value per vertex for each simulation. 

\acro processes all samples of a single edge together. The suggested approach, therefore, decreases the pressure on the memory management unit. On the cascade models, \acro employs vector compute units to its near maximum efficiency to regularize memory accesses.

\item \acro{} can process very large graphs with millions of vertices and hundreds of millions of edges under a minute, without compromising the quality of results. For large graphs, due to its race condition indifferent nature, the performance scales near linearly with the number of threads available. Furthermore, while processing the graph, only a few GBs of memory is used, mostly for storing the graph itself. 

\item We evaluate the performance, memory consumption, and influence score with sketch- and approximation-based state-of-the-art influence maximization algorithms, namely {\sc Skim}~\cite{cohen2014sketch}, {\sc Tim+}~\cite{tim} and {\sc Imm}~\cite{minutoli2019fast}, to accurately position the performance of \acro{} in the IM literature. The experiments show that \acro is $\minspeedup\times$â€”$\maxspeedup\times$ faster than state-of-the-art sketch-based algorithms while having almost the same quality with the approximation-based algorithms in terms of scores, and using a comparable amount of memory.

\end{itemize}

The paper is organized as follows: 
In Section~\ref{sec:background}, we present 
the background on IM and introduce the mathematical notation. 
Section~\ref{sec:method} describes the proposed approach in detail.
In Section~\ref{sec:evaluation}, a thorough performance comparison over the traditional algorithms is provided by conducting experiments on various real-world datasets and influence settings. A detailed comparison with the state-of-the-art from the literature is also given.
Section~\ref{sec:relatedwork} presents a comparative overview of the existing work. Finally, Section~\ref{sec:conclusion} discusses future work and concludes the paper.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Notation and Background}\label{sec:background}

Let $G = (V,E)$ be a directed graph where the $n$ vertices in $V$ represent the agents, and $m$ edges in $E$ represent the relations between the agents in $V$.
The neighborhood of a vertex $u \in V$ is denoted as $\Gamma_{G}(v) = \{v: (u,v) \in E\}$. 
A graph $G' = (V',E')$ is a sub-graph of $G$ if $V' \subseteq V$ and $E' \subseteq E$. The diffusion probability on the edge $(u, v) \in G$ is noted as $w_{u,v}$, where $w_{u,v}$ can be determined either by the diffusion model or according to the strength of $u$ and $v$'s relationship in the data.

\begin{table}[!ht]
    \caption{Table of notations}
    \label{tab:notation}
    \centering
    \begin{tabular}{|l|p{0.7\linewidth}|}
        \hline
        Variable & Definition  \\
        \hline
        $G = (V,E)$     & Graph $G$ with vertices $V$ and edges $E$ \\
        $\Gamma_G(v)$   & Neighborhood of incoming edges to vertex $v$ in graph $G$\\ %%FIXME
        $w_{u,v}$       & Probability of $u$ directly influencing $v$ \\
        %$SCC(v) $       & Strongly connected component of vertex $v$\\
        $R_{G}(v)$      & Reachability set of vertex $v$ on graph $G$\\
%        $\overline{R_{G}(v)} & Complement of the vertex set $R_{G}(v)$\\
        \hline\hline
        $S$             & Seed set to maximize influence\\
        $K$             & Size of the seed set\\
        $\mathcal{R}$   & Number of Monte-Carlo simulations performed\\
        $\sigma_{G}(S)$ & The influence score of $S$ in $G$, i.e., expected number of vertices reached from $S$ in $G$\\
        % $\sigma_{G}{(S,v)}$          & Marginal influence gain by adding vertex $v$ to seed set $S$\\
        \hline\hline
        $w_{u,v}$             & Sampling probability for the edge $(u,v)$\\
        $P(s,v)_r $     & Random probability generated for selecting edge vertices $s$ to $v$ in simulation $r$\\
        $h(u,v)$        & Hash function for edge $\{u,v\}$\\
        $h_{max}$       & Maximum value hash function $h$ can return\\
        %$X_r$           & Random number/hash generated for simulation $r$  \\
        \hline\hline
        $e$             & Estimated reachability set size\\
        % $[a, \ldots, a]_B$      & Vector of size $B$, contains all $a$\\
        $M_u[j]$        & $j$th sketch register for vertex $u$\\
        $\varsigma $    & Influence gained before last sketch build\\
        $\sigma $       & Influence Score\\
        $\delta$        & Marginal Gain after last sketch build\\
        $Err_l$         & Local estimation error of the sketch\\
        $Err_g$         & Global estimation error of the sketch\\
        $\epsilon_{g}$    & Global estimation error threshold\\
        $\epsilon_{l}$    & Local estimation error threshold\\ 
        $\epsilon_{c}$    & Non-convergenced vertex threshold\\
      %  $x[[i, j]]$   & Slice of vector  $x$ between indices $i$ and $j$, not including $j$ \\ 
        \hline         
    \end{tabular}
\end{table}
\subsection{Influence Maximization}

Influence Maximization aims to find a seed set $S \subseteq V$ among all possible size $K$ subsets of $V$ that maximizes an {\em influence spread function} $\sigma$  when the diffusion process is initiated from $S$. Although we focus on graphs with bidirectional edges, for IM, the edges can be unidirectional depending on the initial construction. That is although the edges $\{u, v\} \in E$ are assumed to be bidirectional, throughout the diffusion process, we use $(u, v)$~($(v, u)$) as the edge that may be used to influence/activate $v$~($u$) given that $u$~($v$) has already been influenced/activated. This being said, $w_{u,v}$ can be set to zero to make the edge $(v, u)$ practically unidirectional.

\begin{figure}[!ht] 
    \centering
  \subfloat[\small{IC}\label{fig:ic}]{%
       \includegraphics[width=0.37\linewidth]{images/ic.pdf}}
  \subfloat[\small{WC}\label{fig:wc}]{%
        \includegraphics[width=0.37\linewidth]{images/wc.pdf}}
    \\% IMAGES HERE
  \caption{\protect\subref{fig:ic} 
The directed graph $G = (V, E)$ for Independent Cascade with independent diffusion probabilities. 
\protect\subref{fig:wc}
The directed graph was obtained from the directed one by setting the diffusion probabilities of incoming edges to $1 / |\Gamma_G(v)|$ for each vertex $v \in V$. 
  }
  %\label{fig_ic_model} 
  \label{fig:xx} 
\end{figure}
In the literature, {\em independent} and {\em weighted cascade}~(IC and WC), and 
{\em linear threshold}~(LT)~\cite{kempe2003maximizing} are three widely recognized diffusion models for IM. 

\begin{itemize}[leftmargin=*]
\item An {\bf Independent Cascade} process performs in iterations and activates a vertex $v$  if one of $v$'s (incoming) edges $(u, v)$ is 
used during the diffusion iteration, which happens with the activation probability $w_{u, v}$, given that $u$ has already been influenced in the previous rounds. The activation probabilities are independent (from each other and previous activations) in the {\em independent cascade} model which we concentrate on in this paper, as in Figure~\ref{fig:ic}.
In theory, there can exist parallel and independent $\{u, v\}$ edges in $E$. In practice, they are merged to a single $\{u,v\}$ edge via preprocessing. 

\item The {\bf Weighted Cascade}  model is a variant of the independent cascade that uses the structural properties of vertices to set the edge weights as shown in Figure~\ref{fig:wc}.
The method, as described in~\cite{MixGreedy}, sets $w_{u, v} = 1 / d_v$ where $d_v$ is the number 
of incoming edges of $v$~(which in the original graph is equal to $\Gamma _G(v)$).
Therefore, if $v$ has $\ell$ neighbors activated in the last round, its probability of activation in the new round is $1-( 1-1 / d_v)^\ell$. 

%$ {|E^G_{u,v}|}/{|E^G_{u}|}$ where $|E^G_{u,v}|$ is the number of parallel $\{u, v\}$ edges and $|E^G_{u}|$ is the number of $\{u, .\}$ edges, i.e., all of $u$'s edges in the undirected graph.

\item{\bf Linear threshold} generalizes the independent cascade model and activates the $v$ vertex once the cumulative activation coming from the neighbors of $v$ exceeds the $\theta v$ threshold. 
All $\{u, v\}$~(or $(u, v)$ in the directed graph) edges with active $u$ vertices are taken into account in the process. Vertex $v$ is activated when the total activation probability through these edges exceeds $\theta_v$~\cite{kempe2003maximizing}.  
Therefore, the independent cascade model is a special variant of the linear threshold model with $\theta_v = 0$. 
\end{itemize}

As a generalization of these models, {\bf triggering} has been proposed by Kempe~et~al.~\cite{kempe2003maximizing} to generalize the previous models mentioned above. In this model, each neighbor has a probability to influence the vertex $v$. The diffusion process chooses a random subset of vertices called the {\em triggering set} to activate the vertices at each instance.

The complexity analysis stays consistent for many diffusion models, including {\em Independent Cascade}~(IC), {\em Weighted Cascade}~(WC), and {\em Linear Threshold}~(LT)  models. We concentrate on the independent cascade model in this paper, but the proposed methods are also relevant to other models in the literature.

\subsection{Count-Distinct Sketch}\label{sec:sketch}
The {\em distinct element count} problem focuses on finding the number of distinct elements in a stream with non-distinct elements. Computing the reachability sets of a vertex is a similar problem; the cardinality of all vertices visited is calculated while traversing sample subgraphs from the given vertex. Exact cardinality calculations require memory proportional to the cardinality. 

The reachability set of a vertex is a distinct union of all connected vertices' reachability sets. Many other Influence Maximization methods exploit this property to some degree. 

The methods based on Reverse Reachability and Bottom-Up Traversal utilize this property directly to merge the reachability sets of connected vertices to estimate vertices influence.
MixGreedy method goes one step further; it utilizes that all vertices in a connected component that have the same reachability set in undirected graphs. Therefore, for a sample graph, all reachability sets can be found in a single graph traversal. 

For directed graphs, storing reachability sets for all vertices and merging these sets are infeasible for nontrivial graphs. 
If one-hot vectors are used to store reachability sets for constant insertion time, $O(n^2\mathcal{R})$ bits of memory is required and each merge operation has $O(n)$ time complexity. 
if Disjoints sets are used for storing reachability sets; $(n\bar{\sigma}\mathcal{R})$ memory is required to store all reachability sets, and each merge operation has $O(Ackermann'(\sigma))$ complexity.

Count-distinct sketches have nice properties for such problems; Flajoletâ€“Martin algorithm\cite{flajolet1985probabilistic} can estimate the cardinality of distinct elements in reachability sets with a constant number of registers($J$), and most important, union operation on these sets can be done in a constant number of operations.


Flajoletâ€“Martin algorithm only stores how rare elements are in a multiset. The rarity of the elements is often computed by counting the leading zeros in the item's hash value.  The registers are initialized with zero values in the beginning. Items are added by storing the longest leading zero count. Cardinality estimation can be done by taking 2 to the power of the stored value. Multiple registers and hash functions, $M[j]$ and $h_j$, are commonly used to reduce variance.
Adding the item $x$ to sketch $M$, and its result $M'$ is shown in equation~\ref{eq:sketch-add}.

\begin{equation}
    \label{eq:sketch-add}
    M'[j] = max(M[j],clz(h_j(x)) ~ \forall j\in(0,J]
\end{equation} where $clz$ is count leading zeros function and $J$ is the number of sketch registers.

The merge operation for two sketches, $M_u$,$M_v$, can be performed by computing the pairwise maximum of the registers. Formally; 
\begin{equation}
    \label{eq:sketch-merge}
    M'[j] = max(M_u[j],M_v[j]) ~ \forall j\in(0,J]
\end{equation} 
where $M'$ is union of set and $J$ is the number of sketch registers.

Average of the longest running zeros can be used compute the cardinality and the result is divided to the correction factor $\phi \approx 0.77351$ to correct hash collisions.
\begin{equation}
    \label{eq:sketch-estimate}
    e = 2^{\bar{M}}/\phi
\end{equation} 
where $e$ distinct cardinality and $\bar{M}$ is the mean of the registers.

In this work, we utilize a variant of Flajoletâ€“Martin sketch; since multiple Monte-Carlo simulations are performed to calculate the estimated influence, we use one register per simulation and take the average of longest leading zero counts to calculate the average cardinality of reachability sets. Merge operations are performed if and only if there is an edge between the respective simulations. 

\section{Error-Adaptive Influence Maximization}\label{sec:method}

% \kktodo{burada bir giris gerekiyor - basit, 4-5 adimli bir algoritma, bir iki paragraf anlatim ne yapiyoruz, niye yapiyoruz} % gg:Anlatim ile aciklamayi birlestirdim.
Most influence maximization methods have the same few steps to compute the seed set including; Sampling, building influence oracle, verifying candidates, then removing the residual reachability set of the latent seed set. \acro, on the other hand, fuses sampling step to other steps to accumulate memory accesses.

\acro first performs a diffusion process; The process starts with per-vertex sketches that are initialized with their own vertices. 
Then, for all edges sketch of the source vertex is merged with the target vertex until all registers converge. 
The merge operation utilized in the process is slightly different from the conventional use and retrofitted to mimic the independent cascade diffusion model. 
Registers of a sketch are assigned to a single sample/simulation; Merge operations are performed on the register only if both vertices are $live$ in the sample associated with the register. 
This process relays reachability information between vertices. 
At each iteration, all reachable vertices in a single step are added to the sketches. 
Instead of performing diffusion for a single vertex/path/, all influence that passes through an edge in a single step is processed. 
This dynamic programming like property allows us to estimate marginal influence for all vertices very fast. 
After, \acro picks the vertex with the largest estimated reachability set by looking at sketches alone. 
After, \acro calculates reachability sets of the latent seed set by performing Monte-Carlo simulations. 
The Vertices in the reachability sets are labeled blocked in respective samples. 
Finally, \acro computes whether to rebuild the sketches using the error between sketch estimate and Monte-Carlo estimate.  




\subsection{Hash-based Fused Sampling}
The probabilistic nature of cascade models requires sampling subgraphs $\hat{G}$ from $G = (V, E)$ to simulate the diffusion process. The sampling part of the algorithm can be a dominating factor in processing; sampling may demand multiple passes on the graph, if samples are memoized, multiple times of the graph size needs to be stored.
In this work, we borrow hash-based fused sampling from {\sc InFuseR}\cite{infuser}. By using a hash-based fused sampling, we eliminate the necessity of in-memory creation and storage of the sample subgraphs.
While processing an edge, it is considered for all possible samples.
It is sampled or skipped depending on the outcome of the hash-based random value. The hash function used is given in equation \ref{eq:hash}.
\begin{equation}
    \label{eq:hash}
    h(u,v) = \mbox{{\sc Murmur3}}(u||v)~mod~2^{31}  
\end{equation}
where $||$ is the concatenation operator. 

We have tried a few other hash algorithms as well; Ideally, we chose {{\sc Murmur3}} due to its simplicity and good avalanche behavior with maximum bias $0.5 \% $\cite{MurmurHash3Performance}.

For performance considerations, edge hash values were precomputed in our work and only existing pairs were considered. The trade-off between extra memory and computation here maybe not be applicable for different computation architectures and faster/simpler hash functions.

Although the above-mentioned approach generates a unique hash value for each edge, and hence a unique sampling probability, different simulations require different probabilities.

\begin{figure}[!ht] 
    \centering
    %\subfloat[\label{fig:toy}]{%
    %   \includegraphics[width=0.47\linewidth]{sims-a%}}%  \\
    \subfloat[\label{fig:sims}]{%
        \includegraphics[width=0.5\linewidth]{images/samples.pdf}
    } 
    \subfloat[\label{fig:fused}]{%
     \includegraphics[width=0.5\linewidth]{images/fused.pdf}}
  \caption{
  %\protect\subref{fig:toy} The toy graph 
  \protect\subref{fig:sims} Two sampled subgraphs of the toy graph from Figure~\ref{fig:ic} with 4 vertices and 6 edges.
  \protect\subref{fig:fused} The simulations are performed in a way to be fused with sampling. Each edge is labeled with the corresponding sample/simulation IDs. 
  }
  \label{fig:traversal} 
\end{figure}


Multiple deterministic random values for each edge are generated by using the edge hash values and random numbers $X_r$ associated with each simulation $r$. 
Sampling probability of $(u, v)$ for simulation $r$, $P(u, v)_r$, is computed as follows; First, $h(u,v)$ is XOR'ed with a uniformly randomly chosen $X_r \in_R [0, h_{max}]$ and the result is normalized by dividing the value to the upper limit of the hash value $h_{max}$. Formally,
\begin{equation}
    \label{eq:hash_prob}
    P(u,v)_r = \frac{X_r \oplus h(u,v)}{h_{max}}.
\end{equation}

% The edge $\{u,v\}$ is verified to be in the sample if ${\rho}(u,v)_r$ is smaller than or equal to the threshold $w_{u,v}$. 
The edge $\{u,v\}$ is exists in the sample $r$ if and only if  ${P}(u,v)_r$ is smaller than the edge threshold $w_{u,v}$. One of the benefits of this approach is that an edge can be sampled using a single XOR and compare-greater-than operation. Moreover, the following control flow branch can be removed using conditional move operations. 
An efficient implementation of this approach will be discussed later in this section.
\begin{figure}[!ht] 
    \centering
    \includegraphics[width=1\linewidth]{./images/CDF.pdf}
    \caption{Cumulative probability function of hash-based sampling probabilities on various real-life networks.}
    \label{fig:prob-cdf} 
\end{figure}
Using a strong hash function such as {{\sc Murmur3}} ensures that all bits independently change if the input is changed. This property allows us to generate good enough pseudo-random values for fair sampling. To prove the fairness of random values generated with the hash-based approach, we generated a large number of samples for various real-life networks and graphed the bias of random values $P(u,v)_r$ used while sampling.

% For a given graph $G = (V,E)$, the bias of a sampling probability $x$ is computed as $|(31/2) âˆ’ E(|H(m)|) / E(|H(m)|))|$ for all $(u,v) \in E$ and $0 \leq r < R$. 
% Figure~\ref{fig:prob-bias} shows the bias for 12 real-life networks. 
The sampling probability distribution of hash-based computation is almost identical to the uniform distribution with small bias.


% \begin{figure}[!ht] 
%     \centering
%     \includegraphics[width=1\linewidth]{./images/bias.pdf}
%     \caption{Bias distribution of hash-based sampling probabilities on various real-life networks.}
%     \label{fig:prob-bias} 
% \end{figure}

%Traditional implementations sample each edge in $G$ once per simulation and store them to construct a sampled subgraph. n
Being able to generate infinitely many samples on-the-fly allows us to avoid many memory accesses. The only downside of hash-based fused sampling is that we have to generate all random values,   $P(u,v)_r$, for each edge traversal.  Fortunately, these computations are quite fast on modern computing hardware.

\subsection{Reachability Set Cardinality Estimation}
A greedy solution to the influence maximization problem requires finding a vertex that maximizes the marginal influence gain at each step until the seed set size reaches $K$. And, single vertex influence estimation is the same problem as the count-distinct problem applied to all sample subgraphs. So, it is possible to use Count-distinct sketches to estimate influence scores in the greedy method. 

We propose an influence maximization method that utilizes the Count-distinct sketch we mentioned in section \ref{sec:sketch} to estimate the averages of distinct elements in sample subgraphs for both time and space efficiency. Algorithm \ref{algo:main} describes the method proposed as follows; 

First, the reachability sets of all vertices are initialized with the vertices themselves. For all vertices $u$, its $j$th register is set to $M_u[j]=clz(h_j(u))$ meaning $R_{\hat{G}_j}(u) = \{u\}$. Then, we perform the diffusion process on the sketch registers. 

Second, the diffusion process is performed between vertex sketches. All vertices added to live vertices set $L$. Then, the incoming edges $(u,v)$ for all live vertices are processed. For all simulations where $u$ is live, the sketches of both sides are merged and set to $u$ registers. If any of the sketch registers change, the vertex $u$ is added to the next iteration's live vertices set $L'$. After the live edges are processed, if the next iteration's live vertices set is smaller than the convergence threshold $\epsilon_c$, the diffusion process concluded. If not, we swap $L$ and $L'$, clear $L'$, and then the diffusion processed is repeated. 

In Greedy algorithm\cite{kempe2003maximizing} for each candidate vertex, influence diffusion computations are performed for each vertex.  On the other hand, diffusion on sketch registers allows finding all vertices reachability set. All vertices influence scores in a single-step distance only computed once. This dynamic programming like property allows fast selection of influence maximizing candidates. Only iterations as much as the diameter of sample graphs are required to converge the sketches. 


After diffusion, the following process is repeated until $K$ vertices are added to the seed set; For all vertex, $u$ reachability set cardinalities, $R_G(S\cup u)$, estimated by merging $M_S$ and their sketch registers. The $M_S$ used for the sketch of $R_G(S)$ and initialized with zeros. Vertex with maximum cardinality is selected to add to the seed set. Then, the actual Monte-Carlo simulations were performed to compute the reachability set of the seed set. Having an actual $R_G(S)$ allows both to calculate estimation errors and to find blocked vertices for all simulations. Blocked vertices skipped in the diffusion step in coming iterations. Besides, the real influence score allows us to know when to rebuild our sketches when the sketches have gone stale.


\begin{figure*}[!ht]
    \begin{center}
    \includegraphics[width=0.7\linewidth]{images/sketch-diffusion.pdf}
    \caption{(a) The initial state on the toy graph for \acro{}; all vertices have set as live(green) and their registers initialized with leading zero count of their hashes. (b) 
    % For all live edges,$e_{ij}$, respective registers are merged and set to source vertex $j$'s register, $M_j \leftarrow Max(M_i, M_j)$. 
    For the live edge between Vertex 1 and 4, the register is set to the maximum of the respective register. Since the first register of vertex 1 has a larger value, we only update the second register. Also, the edge between vertex 4 and 3 is only live in simulation 2, so we only set the maximum of those two registers to vertex 4's second register. Notice that we are moving registers in reverse, from target to source of the edges. We set vertices 1 and 4 as live(green) since their registers have changed in this iteration (c) Vertex 1's register is updated and set as live again since vertex 4's registers are changed.  (d) All vertices have registers converged. There are no live vertices. }\label{fig:hf-processing} 
    \end{center}
    \end{figure*}
\subsection{Error-Adaptive Sketch Rebuilding}

Sketches are very fast approximation methods that often provide less than gold standard results. In our case, sketches are very competent at finding the first few vertices. But, after sketch registers saturate with high values, we observe a significant drop in the quality of results. Register saturation causes the naive count-distinct sketch method to not distinguish between vertices. 

\begin{figure}[!ht]
    \begin{center}
    \includegraphics[width=0.7\linewidth]{images/sketch-saturation.pdf}
    \caption{ Effect of register saturation on Amazon dataset using \acro($J=256$) without rebuilding.
     }\label{fig:sketch-saturation} 
    \end{center}
\end{figure}

In figure \ref{fig:sketch-saturation}, we show the effect of register saturation by comparing two methods; one that uses a new sketch, that build on the residual graph after the seed sets' reachability is removed, for all iterations. Moreover, another one that builds the sketch once at the beginning, then only utilizes sketch merge operation to find the seed set. 
Even though at first few vertices the expected influence is comparable, we observe near-linear drop-off in the estimated influence.
    

Since building sketches with \acro is very fast, we exploit this by rebuilding sketches after observing large cardinality estimation errors.
After adding vertices to the seed set, \acro performs Monte-Carlo simulations to estimate the reachability of the latent seed set.
The estimation error is calculated as the following; we store influence score after each sketch rebuild on $\varsigma$ and sketch cardinality estimate of the candidate vertex on $e$. We calculate the marginal gain after the last sketch build, $\delta=\varsigma - \sigma$. Then, the local estimation error will be $Err_l=|e-\delta / \delta|$ and global error rate $Err_g=|e-\delta / \sigma|$. 
The sketches are assumed as stale if the absolute error in the estimated total influence score, $Err_g$, is more than the global threshold, $\epsilon_{g}$, 
and the marginal gain estimation error, $Err_l$, after the last sketch is larger than a local threshold $\epsilon_{l}$.

Before rebuilding stale sketches, the reachability set of the latent seed set, $R_G(S')$, is marked as visited and considered blocked in their respective simulations while rebuilding the sketches. Then, the sketches are rebuilt using Monte-Carlo simulations. After rebuilding, $M_S$, which holds the cardinality estimate of the seed vertices added after the last rebuild, is cleared with all zeros.

The use of two threshold values allows the algorithm to rebuild after large local errors but skip rebuilding if the error is minuscule relative to the total score. 
If a rebuild is not required, we only update $M_S$ by merging it with the candidate vertex's sketch. 
This approach allows us to keep the sketches in high quality even after many vertices are added to the seed set. But on the other hand, it forces us to perform expensive Monte-Carlo simulations. 
We reuse the reachability sets of the latent seed set, $R_G(S')$, computed by Monte-Carlo simulations. While rebuilding the sketches, $R_G(S')$ is used to check whether a vertex is blocked in a sample.

Other methods mentioned in this paper do not report the time spent on calculating the influence scores, 
whereas our method includes the time since it is crucial for keeping the sketches in high quality.

\begin{figure*}[!h]
    \begin{center}
        \subfloat[LiveJournal execution time in seconds\label{fig:lj-time}]{%
        \includegraphics[width=0.33\linewidth]{images/heatmap_LiveJournal_time.pdf}
    }
    \subfloat[Orkut execution time in seconds\label{fig:orkut-time}]{%
        \includegraphics[width=0.33\linewidth]{images/heatmap_Orkut_time.pdf}
        }
    \subfloat[Pokec execution time in seconds\label{fig:pokec-time}]{%
        \includegraphics[width=0.33\linewidth]{images/heatmap_Pokec_time.pdf}
        }\\
        \subfloat[LiveJournal Influence Score\label{fig:lj-score}]{%
        \includegraphics[width=0.33\linewidth]{images/heatmap_LiveJournal_score.pdf}
        }
    \subfloat[Orkut Influence Score\label{fig:lj-score}]{%
        \includegraphics[width=0.33\linewidth]{images/heatmap_Orkut_score.pdf}
        }
    \subfloat[Pokec Influence Score\label{fig:pokec-score}]{%
        \includegraphics[width=0.33\linewidth]{images/heatmap_Pokec_score.pdf}
        }\\
    \end{center}
    \caption{Effect of $\epsilon$ parameters on \acro($J=256$, $\epsilon_c=0.02$) performance, using $\tau=18$ threads.}\label{fig:parameter-smallmultiples} 
\end{figure*}    
\begin{figure}[h] 
    \centering
    \includegraphics[width=1\linewidth]{images/converge-performance.pdf}
  \centering \caption{Geometic mean of \acro's performance and results quality on different $\epsilon_c$ parameters. ($J=256,\epsilon_l=0.3,\epsilon_g=0.01,\tau=18$) 
    \label{fig:conv-perf}} 
\end{figure}

Since $\epsilon_{l}$ is the maximum error allowed after the last rebuild. Setting $\epsilon_{l}$ to $0$ means that \acro will always rebuild sketches, which would slow-down the method, but it will give the best possible results. Conversely, setting $\epsilon_local$ to infinity will be very fast, since sketches are built only once without checking the vertices blocked. But, the result quality will suffer. $\epsilon_{g}$, on the other hand, is the minimum influence we expected to obtain via rebuilding the sketch. If we were to set $\epsilon_{l}$ to 0, \acro will not rebuild the sketches, but when $\epsilon_{l}$ is set to infinity, we skill need to compare $\epsilon_{l}$ to the estimation error before rebuilding.

Even though \acro's behavior is not chaotic with respect to changes in parameters. The performance, both in terms of speed and quality of the results, depends on parameter selection. We conducted a grid search over several parameter combinations to illustrate how \acro performs with different parameters. Both the speed and the quality of the results on some of the largest datasets we have experimented shown in figure~\ref{fig:parameter-smallmultiples}.  We found that the parameters $\epsilon_{l}=0.3$ and $\epsilon_{global}=0.01$, used in our experiments, perform well in many datasets, both in terms of speed and quality.


\begin{algorithm}
\caption{\sc{\acro}($G,K,J$)}
\label{algo:main}
\algorithmicrequire{$G = (V,E)$: the influence graph
\\\hspace*{6.6ex}{$K$: number of seed vertices
\\\hspace*{6.7ex}$\mathcal{J}$: number of MC simulations}\\}
\algorithmicensure{$S$: a seed set that maximizes influence on $G$
}
\begin{algorithmic}[1]
    \State {$S \leftarrow \{\emptyset\}$}
    \For{$ v\in V$} {\bf in parallel}
        \For{$ j\in J$}
            \State $M[j]_v \leftarrow clz(hash(v) \oplus hash(j))$ 
        \EndFor
    \EndFor
    \State $M \leftarrow simulate(G,M,\emptyset)$
    \State $M_{S'} \leftarrow zeros(J)$
    \State $\varsigma \leftarrow 0$
    \For{$k=1\ldots K$}
        \State $s \leftarrow \underset{v\in V}{\mathrm{argmax}} ~estimate(merge(M_{S'},M_v))$
        \State $S \leftarrow S \cup \{s\}$
        \State $e \leftarrow estimate(merge(M_{S'},M_s))$
        % \State $R_S \leftarrow run\_cascade(G,S,J)$
        \State {Compute seed set $S$'s Reachability set $R_{G}(S)$ }
        \State $\sigma_G(S) \leftarrow |R_{G}|/J$
        \State $\delta = \sigma - \varsigma$
        \State $Err_l=|(e - \delta) / \delta|$
        \State $Err_g=|e-\delta| / \sigma $
        \If{$ Err_l < \epsilon_l \lor Err_g < \epsilon_g$} %FIXME
            \State $M_{S'} \leftarrow merge(M_{S'},M_s)$
        \Else
            \For{$ v\in V$} {\bf in parallel}
                \For{$ j\in J$}
                    \State $M[j]_v \leftarrow clz(hash(v) \oplus hash(j))$ 
                \EndFor
            \EndFor
            \State $M \leftarrow simulate(G,M,R_S)$
            \State $M_{S'} \leftarrow zeros(J) $ 
            \State $\varsigma \leftarrow \sigma $ 
        \EndIf
    \EndFor
    \State \Return $S$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
\caption{\sc{Simulate}($G,M,J,R_S$)}
\label{algo:diffusion-step}
\algorithmicrequire{$G = (V,E)$: the influence graph
\\\hspace*{6.6ex}{$M$: Sketch vectors of vertices
\\\hspace*{6.7ex}$\mathcal{J}$: number of MC simulations
\\\hspace*{6.7ex}$R_S$: Reachability set of the seed set
}
\\}
\algorithmicensure{$M$: Updated Sketch vectors
}
\begin{algorithmic}[1]
    \State {$L \leftarrow V$}
    \State {$L' \leftarrow {\emptyset}$}
    \While{$|L|/|V| > \epsilon_c$}
        \For{$u \in \Gamma(L)$} {\bf in parallel} \label{ln:inner_start} %REVERSE THIS
        \For{$e_{u,v} \in A(u) $}
            \For{$j \in (0,J]$}
                \If{$P(u,v)_j < w_{u,v} \land u  \not\in R_S[j]$} \label{ln:early_exit}
                    \State{$M_u[j]\leftarrow max(M_u[j],M_v[j])$}\label{ln:update}         
                \EndIf
            \EndFor
            \If {$M_u$ changed}
                \State $L' \leftarrow L' \cup u $ \label{ln:inner_end}
            \EndIf
        \EndFor
        \EndFor
        \State $L \leftarrow L'$
        \State $L' \leftarrow \{\emptyset\}$
    \EndWhile
    \State \Return $M$
\end{algorithmic}
\end{algorithm}

\subsection{Regularization and Performance considerations}
Traditional two-step sample-then-diffuse computation model stores data loosely-coupled. Sampled edges are stored together, diffusion related information stored afar. This way, SIMD operation is allowed on consecutively located edges, requiring random memory accesses for the diffusion process. Even if the sample or diffusion information are stored closely, without fusing, the random access overhead will be still there and much memory will be wasted. 

One of the main advantages of fused sampling is allowing similar simulation data and computations together. For processing samples for an edge in algorithm \ref{algo:diffusion-step}~(lines~\ref{ln:inner_start}--\ref{ln:inner_end}).  

 All memory registers from different simulations are stored together for each vertex, this allows the use of vectorized computation hardware. Random number generation, sampling, and merging sketches are all vectorizable operations when coupled together. Vectorization also reduces branching on the overall computation; When vectorized, many comparison operations are done without branching. 

We allow a single easy to predict branch after sample edges are generated to exit early without merging the registers. In our experiments, sorting random values $X_r$ significantly increases the performance by clustering similar simulations together. Since sorted random values are XOR'ed with the same edge hash values, it is more likely to nontaken edges are sampled together. Up to 20\% speed-up can be achieved by an early exit at line \ref{ln:early_exit} using sorted random values.

\subsection{Implementation Details}
While implementing the \acro, we have used the Compressed Sparse Row~(CSR) graph data structure.
In CSR, an array, $index$, holds the starting indices of each neighboring vertex, while another vector, $adj$, holds the consecutive neighbors of each vertex. 
To enter the neighbors of vertex $i$ we first visit $index[i]$ and $index[i+1]$ 
to find the data's start and end positions, then search from $adj[index[i]]$ to $adj[index[i+1]]$. 
    
\section{Evaluation}\label{sec:evaluation}
We performed the experiments on a server with {\tt Intel Xeon Gold 6140}, running at 2.3 Ghz and 250 GB memory. All of the 18 cores are allowed in the experiments. The Operating System on the server was {\tt Ubuntu 16.04 LTS} with 5.4.0-48 kernel. The algorithms are implemented using {\tt C++20}, and compiled with {\tt GCC 9.2.0} with {\tt "-Ofast"} and {\tt "-march=native"} optimization flags. Multi-thread parallelization was achieved with {\tt OpenMP} pragmas. {\tt AVX2} instructions are utilized by handcrafted code with vector intrinsics.

\begin{table}[!ht]
\caption{Properties of networks used in the experiments}\label{tab:NetProps}
\centering
\scalebox{0.95}{
\begin{tabular}{ll||r|r|r|r}
& & No. of           & No. of    & Avg.  &  Avg.  \\
&Dataset & Vertices          & Edges     &    Weight         &  Degree               \\
\hline
\multirow{6}{*}{\rotatebox[origin=c]{90}{Undirected}}& {\tt Amazon} & 262,113 & 1,234,878 & 1.00 & 4.71 \\
&{\tt DBLP} & 317,081 & 1,049,867 & 1.00 & 3.31 \\
&{\tt NetHEP} & 15,235 & 58,892 & 1.83 & 3.87 \\
&{\tt NetPhy} & 37,151 & 231,508 & 1.28 & 6.23 \\
&{\tt Orkut} & 3,072,441 & 117,185,083 & 1.00 & 38.14 \\
&{\tt Youtube} & 1,134,891 & 2,987,625 & 1.00 & 2.63 \\

\hline
\multirow{6}{*}{\rotatebox[origin=c]{90}{Directed}}&{\tt Epinions} & 75,880 & 508,838 & 1.00 & 6.71 \\
&{\tt LiveJournal} & 4,847,571 & 68,993,773 & 1.00 & 14.23 \\
&{\tt Pokec} & 1,632,803 & 30,622,564 & 1.00 & 18.75 \\
&{\tt Slashdot0811} & 77,360 & 905,468 & 1.00 & 11.70 \\
&{\tt Slashdot0902} & 82,168 & 948,464 & 1.00 & 11.54 \\
&{\tt Twitter} & 81,306 & 2,420,766 & 1.37 & 29.77 
\end{tabular}
}
\end{table}
\subsection{Experiment Settings}

We performed experiments on twelve graphs~(six undirected, six directed) are used in the experiments. For relevancy, datasets that are frequently used for Influence Maximization are selected.

The datasets are {\tt Amazon} co-purchase network~\cite{snapnets}, {\tt DBLP} co-laboration network~\cite{snapnets}, {\tt Epinions} consumer review trust network, {\tt LiveJournal}~\cite{snapnets}, {\tt NetHEP} citation network~\cite{MixGreedy}, {\tt NetPhy} citation network~\cite{MixGreedy}, {\tt Orkut}~\cite{snapnets}, {\tt Pokec} Slovakian poker game site friend network~\cite{snapnets}, {\tt Slashdot} friend-foe networks~(08-11, 09-11)~\cite{snapnets}, {\tt Twitter} list co-occurence network~\cite{snapnets}, and {\tt Youtube} friendship network~\cite{snapnets}. The properties of these datasets are given in Table~\ref{tab:NetProps}. 

The properties of these datasets are given in Table~\ref{tab:NetProps}. 

Three diffusion settings are simulated for a comprehensive experimental evaluation; for each network, we use 
\begin{enumerate}
    \item constant edge weights $w = 0.005$,
    \item constant edge weights $w = 0.01$~(as in~\cite{kempe2003maximizing} and~\cite{MixGreedy}),
    \item constant edge weights $w = 0.1$~(as in~\cite{kempe2003maximizing}),
    % \item weighted cascade edge weights $w_{u,v} = 1/|\Gamma(v)|$.
\end{enumerate}

We selected $w=0.005$ as one of our benchmark settings to challenge ourselves; due to the nature of the diffusion algorithm, \acro traverses vertices even if they are blocked. Also, for each live vertex, \acro generates sample edges for all simulations regardless. The other two experiment settings are used to emulate the experiments of Kempe et al.~\cite{kempe2003maximizing}

\subsection{Performance Metrics}

In this paper, the algorithms are evaluated on execution time, influence score, and maximum resident memory . In computing, these three are
often traded off to each other; in the extreme, it is trivial to select random vertices as the seed set or compute reachability sets of every possible seed set combination. 

The observed execution times and memory consumption of different algorithms are comparable when the algorithms run on the same computer.
In this paper, the execution times are always the wall time reported by the programs.
All methods we benchmarked leave out the time spent on reading files and preprocessing. To be fair in evaluating our method, we only left out the time to spend on reading files. 

In all benchmarks, we allowed all methods to fully utilize all available CPU cores, except {\sc Tim+} which is a single-threaded software. Memory use we reported is Resident Set Size(RSS) and measured using GNU Time software.

Although, because the algorithms can use various methods to measure the impact score, the reported influence scores may be deceptive. 
Due to this reason, we implemented an oracle with a very simple, sample-than-diffuse algorithm without any optimization mentioned. The random oracle values are generated by the 32-bit Mersenne Twister pseudo-random generator with a state size of 19937 bits {\tt mt19937} from {\tt C++ standard library}. All influence scores in this paper are reported by the independent oracle.


\subsection{Algorithms evaluated in the experiments}

We evaluated our method against three other state-of-the-art influence maximization algorithms, {\sc Tim+}, {\sc Skim}, and {\sc Imm}.

First, Two-phased Influence Maximization({\sc Tim+})~\cite{tim},TIM consists of two phases, Parameter Estimation and Node Selection. The first phase computes a lower-bound
of the maximum expected to spread among all size-k node sets,
and then uses the lower bound to derive a parameter $\theta$. In the second phase, samples $\theta$ random RR sets from
G, and then derives a size-k node-set $\hat{S}$ that covers a large number of RR sets.

Second, Sketch-based Influence Maximization({\sc Skim})~\cite{cohen2014sketch} constructs bottom-$K^2$ min-hash sketches to estimate the reachability and utilizes multithread parallelization in some degree. 

Third, Minutoli et al.'s {\sc Imm}~\cite{minutoli2019fast}. 
{\sc Imm} is a fast algorithm that efficiently produces accurate seed sets. 
The {\sc Imm}  algorithm\cite{tang2015influence} is an approximation method that improves the Reverse Influence Sampling(RIS)\cite{borgs2014maximizing} algorithm By eliminating the need for the threshold to be used.

The influence of all vertices is generalized on a small set of sample graphs that is sampled from the reachability sets of a few vertices.  Experimentations are done with $\epsilon = 0.5$ as suggested in the original paper, where $\epsilon$ is a user-defined hyper-parameter controlling the approximation boundaries.

Finally, our method \acro employs both Monte-Carlo simulations and count-distinct sketches for estimating reachability set cardinalities. Even though \acro is blazing-fast and accurate for limited $K$, we exploit the performance of the method to improve the results by rebuilding sketches after removing residual influence networks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{table*} %TABLE 5+7
    \caption{\acro execution times~(in secs), memory use~(in GBs), and influence scores on the networks with $K = 50$ seeds using $\tau=18$ threads and constant edge weights $w=0.005$. Influence scores are given relative to \acro{}. }
    \label{tab:timings005}
    \centering
    \scalebox{0.96}{


\begin{tabular}{l|rrrr|rrrr|rrrr}
\toprule
{} & \multicolumn{4}{c|}{Time} & \multicolumn{4}{c|}{Score} & \multicolumn{4}{c}{Memory} \\
Method & {\sc Hyper} &    {\sc Tim+} &    {\sc Imm} &  {\sc Skim} & {\sc Hyper} &    {\sc Tim+} &     {\sc Imm} &    {\sc Skim} & {\sc Hyper} &  {\sc Tim+} &  {\sc Imm} & {\sc Skim} \\

Dataset      & {\sc Fuser} &         &         &         & {\sc Fuser} &         &         &         & {\sc Fuser} &       &       &      \\

\midrule
{\tt Amazon}       &       1.30 &  124.38 &   5.58 & 63.73 &       96.9 &  1.041x &  1.000x &  0.562x &       0.17 & 21.62 & 0.90 & 6.78 \\
{\tt DBLP}         &       1.61 &  177.99 &   5.71 & 28.24 &      106.6 &  1.068x &  1.027x &  1.036x &       0.27 & 31.24 & 0.95 & 3.12 \\
{\tt Epinions}     &       1.11 &   12.16 &   0.50 &  8.29 &      635.3 &  1.026x &  1.001x &  0.939x &       0.06 &  0.78 & 0.07 & 1.04 \\
{\tt LiveJournal}  &      13.25 & 4172.69 & 118.82 & 19.35 &    37174.1 &  1.010x &  0.995x &  0.957x &       3.97 & 27.43 & 2.78 & 2.00 \\
{\tt NetHEP}       &       0.31 &    2.22 &   0.31 &  1.96 &       80.2 &  1.065x &  0.993x &  0.871x &       0.01 &  0.80 & 0.04 & 0.33 \\
{\tt NetPhy}       &       0.39 &    7.40 &   0.40 &  1.00 &      124.5 &  1.042x &  0.999x &  0.777x &       0.03 &  2.01 & 0.08 & 0.16 \\
{\tt Orkut}        &      30.22 &       - & 780.22 & 41.82 &   158842.6 &       - &  0.997x &  1.001x &       5.19 &     - & 7.81 & 1.77 \\
{\tt Pokec}        &      11.05 &  149.34 &   5.04 & 18.40 &     1095.1 &  1.032x &  1.027x &  0.925x &       1.57 & 10.16 & 1.40 & 1.74 \\
{\tt Slashdot0811} &       1.18 &    6.93 &   0.40 &  1.00 &      576.4 &  1.015x &  0.983x &  0.942x &       0.06 &  0.76 & 0.09 & 0.13 \\
{\tt Slashdot0902} &       1.14 &    6.96 &   0.40 &  1.17 &      610.5 &  1.022x &  0.998x &  0.953x &       0.06 &  0.71 & 0.08 & 0.14 \\
{\tt Twitter}      &       1.10 &  171.17 &   5.40 &  1.60 &     3458.7 &  1.006x &  0.990x &  0.942x &       0.09 &  2.02 & 0.12 & 0.15 \\
{\tt Youtube}      &       1.95 &   46.61 &   2.42 & 13.24 &     1820.8 &  1.025x &  1.013x &  1.000x &       0.73 &  3.96 & 0.48 & 1.39 \\
\bottomrule
Geo. Mean wrt.&1.000x&28.191x&1.687x&3.465x&1.000x&1.032x&1.002x&0.899x&1.000x&22.750x&1.650x&3.581x \\ \acro
\end{tabular}

    }
    \end{table*}
    
    \begin{table*} %TABLE 5+7
    \caption{\acro execution times~(in secs), memory use~(in GBs), and influence scores on the networks with $K = 50$ seeds using $\tau=18$ threads and constant edge weights $w=0.01$. Influence scores are given relative to \acro{}. }
    \label{tab:timings01}
    \centering
    \scalebox{0.93}{

% \begin{tabular}{lrrrrrrrrrrrr}
% \toprule
% {} & \multicolumn{4}{l}{Time} & \multicolumn{4}{l}{Score} & \multicolumn{4}{l}{Memory} \\
% Method & HyperFuser &    TIM+ &     IMM &   SKIM & HyperFuser &    TIM+ &     IMM &    SKIM & HyperFuser &  TIM+ &   IMM & SKIM \\
\begin{tabular}{l|rrrr|rrrr|rrrr}
\toprule
{} & \multicolumn{4}{c|}{Time} & \multicolumn{4}{c|}{Score} & \multicolumn{4}{c}{Memory} \\
Method & {\sc Hyper} &    {\sc Tim+} &    {\sc Imm} &  {\sc Skim} & {\sc Hyper} &    {\sc Tim+} &     {\sc Imm} &    {\sc Skim} & {\sc Hyper} &  {\sc Tim+} &  {\sc Imm} & {\sc Skim} \\

Dataset      & {\sc Fuser} &         &         &         & {\sc Fuser} &         &         &         & {\sc Fuser} &       &       &      \\
\midrule
{\tt Amazon }       &       0.96 &  107.94 &    3.28 &  59.32 &      152.7 &  1.037x &  1.024x &  0.390x &       0.17 & 18.11 &  0.55 & 6.40 \\
{\tt DBLP }         &       0.73 &   73.37 &    2.85 &  18.71 &      233.5 &  1.043x &  1.005x &  0.997x &       0.27 & 11.92 &  0.52 & 2.05 \\
{\tt Epinions }     &       0.82 &  112.08 &    3.78 &   5.07 &     2480.1 &  1.006x &  0.983x &  0.984x &       0.06 &  1.95 &  0.10 & 0.68 \\
{\tt LiveJournal }  &      16.72 &       - &  386.37 &  16.23 &   155375.8 &       - &  0.996x &  0.993x &       3.97 &     - &  6.64 & 1.45 \\
{\tt NetHEP }       &       0.26 &    1.84 &    0.23 &   1.89 &      129.1 &  1.036x &  0.997x &  0.826x &       0.01 &  0.60 &  0.03 & 0.31 \\
{\tt NetPhy }       &       0.24 &    3.33 &    0.23 &   0.86 &      320.5 &  1.010x &  0.985x &  0.732x &       0.03 &  0.67 &  0.05 & 0.12 \\
{\tt Orkut }        &      42.37 &       - & 1870.35 & 114.82 &   650157.1 &       - &  1.000x &  1.000x &       5.19 &     - & 20.13 & 3.43 \\
{\tt Pokec }        &      11.65 & 4148.03 &   88.89 &   7.25 &    44685.8 &  1.004x &  0.996x &  0.988x &       1.57 & 39.98 &  2.09 & 0.78 \\
{\tt Slashdot0811 } &       0.84 &  102.96 &    3.70 &   0.87 &     2882.1 &  1.003x &  0.984x &  0.976x &       0.06 &  2.01 &  0.10 & 0.08 \\
{\tt Slashdot0902 } &       0.90 &  129.31 &    4.19 &   0.88 &     3061.5 &  1.008x &  0.992x &  0.980x &       0.06 &  2.42 &  0.11 & 0.08 \\
{\tt Twitter }      &       0.90 &  390.36 &   10.96 &   1.22 &     9628.6 &  1.004x &  0.992x &  0.978x &       0.09 &  4.91 &  0.23 & 0.09 \\
{\tt Youtube }      &       2.18 &  534.41 &   14.86 &  16.97 &     9042.7 &  1.009x &  0.994x &  1.006x &       0.73 &  7.10 &  0.48 & 1.73 \\
\bottomrule
Geo. Mean wrt.&1.000x&100.140x&5.448x&3.582x&1.000x&1.016x&0.996x&0.879x&1.000x&36.264x&1.909x&2.820x \\ \acro
\end{tabular}



    }
    \end{table*}
    \begin{table*} %TABLE 5+7
    \caption{\acro execution times~(in secs), memory use~(in GBs), and influence scores on the networks with $K = 50$ seeds using $\tau=18$ threads and constant edge weights $w=0.1$. Influence scores are given relative to \acro{}. }
    \label{tab:timings1}
    \centering
    \scalebox{0.90}{

\begin{tabular}{l|rrrr|rrrr|rrrr}
\toprule
{} & \multicolumn{4}{c|}{Time} & \multicolumn{4}{c|}{Score} & \multicolumn{4}{c}{Memory} \\
Method & {\sc Hyper} &    {\sc Tim+} &    {\sc Imm} &  {\sc Skim} & {\sc Hyper} &    {\sc Tim+} &     {\sc Imm} &    {\sc Skim} & {\sc Hyper} &  {\sc Tim+} &  {\sc Imm} & {\sc Skim} \\

Dataset      & {\sc Fuser} &         &         &         & {\sc Fuser} &         &         &         & {\sc Fuser} &       &       &      \\
\midrule
{\tt Amazon}       &       0.69 &  133.22 &    1.98 &  23.62 &    11797.0 &  1.006x &  0.990x &  0.815x &       0.17 &   5.49 &  0.23 & 2.59 \\
{\tt DBLP }         &       0.54 & 1368.83 &   14.54 &   6.30 &    48549.9 &  1.001x &  0.995x &  1.001x &       0.27 &  35.19 &  1.06 & 0.65 \\
{\tt Epinions }     &       0.26 &  439.33 &    5.46 &   9.42 &    18409.9 &  1.000x &  0.998x &  0.997x &       0.06 &  12.17 &  0.39 & 1.18 \\
{\tt LiveJournal }  &      10.10 &       - & 1071.30 &  65.73 &  2134726.0 &       - &  1.000x &  1.000x &       3.97 &      - & 65.49 & 1.40 \\
{\tt NetHEP }       &       0.10 &   14.18 &    0.33 &   0.61 &     2461.7 &  1.002x &  0.975x &  0.899x &       0.01 &   1.02 &  0.04 & 0.10 \\
{\tt NetPhy }       &       0.16 &  107.52 &    1.53 &   0.34 &     8339.5 &  1.007x &  0.994x &  0.975x &       0.03 &   3.84 &  0.13 & 0.03 \\
{\tt Orkut }        &      16.55 &       - & 1964.83 & 446.92 &  2692366.5 &       - &  1.000x &  1.000x &       5.19 &      - & 71.94 & 9.68 \\
{\tt Pokec }        &       4.90 &       - &  514.79 &  31.80 &  1034859.8 &       - &  1.000x &  1.000x &       1.57 &      - & 26.46 & 0.98 \\
{\tt Slashdot0811 } &       0.21 &  677.49 &    7.34 &   2.44 &    25871.8 &  1.000x &  1.000x &  0.999x &       0.06 &  19.10 &  0.59 & 0.25 \\
{\tt Slashdot0902 } &       0.23 &  695.12 &    7.99 &   2.35 &    27519.5 &  1.000x &  1.000x &  0.999x &       0.06 &  18.45 &  0.66 & 0.24 \\
{\tt Twitter }      &       0.33 & 1897.50 &   16.09 &   1.62 &    55327.3 &  1.000x &  0.998x &  0.998x &       0.09 &  34.56 &  1.04 & 0.05 \\
{\tt Youtube }      &       1.12 & 7158.56 &   60.59 &  30.57 &   171392.9 &  1.000x &  0.999x &  1.001x &       0.73 & 139.12 &  4.19 & 2.88 \\
\bottomrule
Geo. Mean wrt.&1.000x&1449.408x&27.870x&11.062x&1.000x&1.002x&0.996x&0.972x&1.000x&163.771x&7.145x&2.634x \\ \acro
\end{tabular}


    }
    \end{table*}
    
 \begin{figure}[!h] 
     \centering
     \includegraphics[width=1\linewidth]{images/speedup-imm}
   \centering \caption{Speed-up obtained by \acro{} over {\sc Imm}($\epsilon\myeq 0.5$) using $\tau=18$ threads.
     \label{fig:vs-imm}} 
 \end{figure}

 \begin{figure}[!h] 
     \centering
     \includegraphics[width=1\linewidth]{images/speedup-skim}
   \centering \caption{Speed-up obtained by \acro{}($J=256$) over {\sc Skim}($r\myeq 64,l\myeq 64$) using $\tau=18$ threads.
     \label{fig:vs-skim}} 
 \end{figure}


\subsection{Scalability with multi-threaded parallelism}

\begin{figure*}[!ht] 
    \centering
    \includegraphics[width=\linewidth]{images/threads.pdf}
   \centering \caption{\acro scaling with multiple threads on some of largest datasets in the benchmarks.
     \label{fig:scaling}} 
\end{figure*}


In our implementation, the parallel processing of live vertices  (as source) is necessary to reduce the number of visited edges. However, since a (target) vertex can be being processed at the same time, the update operation at the line~\ref {ln:update} of this {\em pull-based} approach can be seen as a potential source of race conditions. However, since iterations are performed until convergence, the results are sound within our assumptions. On the other hand, the performance may suffer due to false sharing.
Figure~\ref{fig:scaling} shows the speedup values obtained via {\tt OpenMP} parallelization.

Even though {\em pull-based} diffusion approach we employed in this work gives us superior performance in most of the benchmarks, it is possible to implement \acro using other approaches; E.g. {\em queue-based} approach is similarly feasible and may improve performance by only processing live vertices. The pull-based diffusion method is chosen due to its simplicity and scalability to a large number of threads/compute units. 

\subsection{Comparing \acro with State-of-art}
All algorithms mentioned in this paper estimate the influence maximizing seed sets that are good enough but not optimal. For that reason, we compare both the performance and quality of the results. In addition to this trade-off, the parameters of the experimented methods are hard to compare directly. It is infeasible to find exactly the corresponding parameter between any two methods in this setting. For that reason, we have used the parameters suggested by the  authors themselves.
We performed experiments using following parameters; {\sc Tim+} ($\epsilon=0.3$), {\sc Imm} ($\epsilon=0.5$), {\sc Skim} ($l=64,k=64$). 

One of the drawbacks of \acro is that we do not have direct control over the approximation factor, whereas the other methods have some form of an approximation control parameter. \acro can only control approximation quality indirectly by tuning the number of Monte-Carlo simulations $J$.

For \acro, we used $J=256$ registers which is comparable in terms of quality to other methods. Also, we used the global estimation error threshold $\epsilon_{g}=0.01$, local estimation error threshold $\epsilon_l=0.3$, and set non-converged vertices ratio as $\epsilon_{c}=0.02$. 


For small $w=0.005$ and small graphs such as NetHep, NetPhy, or Amazon; we can see {\sc Tim+}  has ($4\sim 6\%$) better results, whereas {\sc Skim}  has much ($- 13\sim 44\%$) worse results than \acro in terms of quality. While \acro is having a hard time to catch edges in samples; \acro has a chance of capturing an edge is $1-(1-0.005)^{256}=0.72$, {\sc Tim+} won't suffer from the situation but {\sc Skim}  suffer much worse capturing vertices in sketches. On the performance side, \acro is superior to other methods, Amazon dataset only takes 1.3 seconds, including Monte-Carlo influence oracle computations. {\sc Skim} and {\sc Imm} take 63.73 and 124.38 seconds. For 4\% quality improvements, {\sc Imm} is $\sim$100 slower than \acro. For small improvements in quality, we can always double the number of registers/simulations $J=256$ to $512$, which would approximately double the wall time. For the same $w=0.005$ and larger graphs such as {\tt LiveJournal} and {\tt Orkut}, the quality gap narrows {\sc Skim}  only $5\%$ behind, and {\sc Tim+} is marginally better. 
The performance gap also narrows between \acro and {\sc Skim}, \acro only $30\%$ faster in small $w$. In the same setting, {\sc Tim+}  takes 1.5 hours for {\tt LiveJournal}, could not finish {\tt Orkut}; crashing after consuming all memory available. 
For $w=0.1$, \acro is much faster than all other methods and still provides high-quality results. While all methods are struggling with problem size, \acro is converging results much faster, even faster than previous experiment settings. Also, the memory consumption stays the same for all experimental settings, much smaller than other methods.

Performance characteristics of \acro are very different from its state-of-the-art competitors. \acro's performance directly depends on the diameter of the influence graph, whereas other methods mention in this paper depends on the influence score. Due to this reason, we observe highly varying performance speed-up results in tables \ref{tab:timings005}-\ref{tab:timings1}. In exceptional settings, such as the Pokec dataset and $w=0.01$ where the diameter of the influence network is around 43, \acro loses its edge against its fastest competitor. On the other hand, in the same dataset with $w=0.1$, the diameter is only around 17; \acro is 6 times faster than its nearest competitor.

\section{Related Work}\label{sec:relatedwork}

Although they can be inferior in terms of influence score, recent IM algorithms are shown to be quite fast compared to conventional simulation-based approaches such as {\sc MixGreedy}. 

Techniques such as using GPUs\cite{IMGPU,curipples}, sketches for finding set intersections\cite{cohen2014sketch,IPA}, reverse sampling to estimate influence from a small subset of vertices\cite{borgs2014maximizing,minutoli2019fast}, and estimating the necessary number of simulations/samples required for each step greatly reduces asymptotic boundaries of execution time\cite{leskovec2009community}.

\acro borrows much from {\sc InFuseR}~\cite{infuser}, including hash-based fused sampling and vertex parallel diffusion algorithm. {\sc InFuseR} computes influence by memoizing connected-components for all vertices and only works on {\em undirected} datasets. In addition, it employs CELF optimization to reduce candidate seed set cardinality computations. Whereas, \acro can process both directed and undirected graphs, and uses sampled Flajoletâ€“Martin sketches to both estimate cardinality and choose candidates. 

Sketch-based influence maximization improves the theoretical efficiency against simulation-based methods. The sketch-based approach pre-computes sketches for evaluating the influence spread instead of running simulations repetitively. One of the interesting methods for sketch-based influence maximization is {\sc Skim} ~\cite{cohen2014sketch} by Cohen~et~al. It constructs bottom-$K^2$ min-hash sketches to estimate the reachability and utilizes multi-core/multi-CPU parallelization. 

Independent Path Algorithm~(IPA)~\cite{IPA} by Kim~et~al, runs a proxy model and prunes paths with probabilities less than a given threshold. IPA uses {\tt OpenMP} to work on independent paths in parallel. The approach only keeps a dense but small part of the network and scalable to only sparse networks. Liu~et~al. proposed IMGPU~\cite{IMGPU}, an IM  estimation method by utilizing a bottom-up traversal algorithm. It performs a single Monte-Carlo simulation on many GPU threads to find the reachability of the seed set. It is $5.1\times$ faster than {\sc MixGreedy} on a CPU. The GPU implementation is up to $60\times$ faster with an average speed-up of  $24.8\times$.

Borgs~et~al.~\cite{borgs2014maximizing} proposed Reverse Influence Sampling~(RIS), which samples a fraction of all random reverse reachable sets. Then it computes a set of $K$ seeds that covers the maximum number of those. The number of samples is calculated concerning the number of visited vertices. The algorithm has an approximation guarantee of $(1-1/e-\epsilon)$. Minutoli et al.~improved RIS and proposed {\sc Imm} that works on multi-threaded and distributed architectures~\cite{minutoli2019fast}. 
Recently, the authors extended the algorithm to work on GPUs~\cite{curipples}.

Two-phased Influence Maximization({\sc Tim+})\cite{tim} borrows ideas from RIS but overcomes its limitations with a novel algorithm design. The first phase computes a lower-bound of the maximum expected to spread among all size-k node sets. Then, it uses the lower bound to derive a parameter $\theta$. In the second phase, samples $\theta$ random RR sets from G, and then derives a size-k node-set $\hat{S}$ that covers a large number of RR sets.

Cohen\cite{cohen2015all} presented the Historic Inverse Probability (HIP) estimators. When applied to All Distance Sketches, HIP outperforms in the estimation quality of the HyperLogLog sketches\cite{flajolet2007hyperloglog} for counting distinct elements on data streams. In this work, we preferred Flajolet-Martin\cite{flajolet1985probabilistic} sketches for simplicity and performance. In future work, other estimators such as HIP can improve to improve \acro.

Kumar and Calders\cite{kumar2017information} proposed Time Constrained Information
Cascade Model and an influence maximization algorithm that works on the model using versioned HyperLogLog sketches. The algorithm computes the influence reachability sets for all vertices in an interaction graph while performing a single pass over the data. Sketches are used for each time window to estimate active edges, whereas \acro builds sketches for each vertex to estimate the cumulative influence of the latent seed set. \acro also utilizes fused sampling and error adaptive rebuilding of sketches.

\section{Conclusion and Future Work}\label{sec:conclusion}

In conclusion, we propose a sketch-based Influence Maximization algorithm that employs fused sampling and error-adaptive rebuilding. We provide a fast implementation of the algorithm that utilizes multi-threading to exploit multiple cores. Also, we present a performance comparison with state-of-the-art IM algorithms on real-world datasets. 
With the proposed method, \acro can achieve $\maxspeedupSKIM\times$ speed-up against {\sc Skim} and $\maxspeedupIMM\times$ speed-up against {\sc Imm} while producing high quality results. 

In the future, we would like to extend our work to a distributed GPGPU setting to process graphs with billions of vertices and edges. The main issue with \acro on such computation scheme is the broadcast of sketch registers; when vertices are partitioned to different computation units, resulting sketch updates should be communicated to all partitions. To overcome the issue, graph preprocessing and speculative diffusion strategies can reduce communication bottlenecks. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\ifCLASSOPTIONcaptionsoff
  \newpage
\fi


\bibliographystyle{IEEEtran}
\bibliography{refs}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./images/gokhan.png}}]{G\"{o}khan~G\"{o}kt\"{u}rk} is a PhD candidate at the Faculty of Engineering and Natural Sciences in SabancÄ± University. He has received his BS and MS degrees from SabancÄ± University as well. He is interested in High Performance Computing, Parallel Programming, and Graph Processing.
\end{IEEEbiography}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./images/kamer.jpg}}]{Kamer Kaya} is an Assistant Professor at the Faculty of Engineering and Natural Sciences at SabancÄ± University. He got his PhD from Dept. Computer Science and Engineering from Bilkent University. He worked at CERFACS, France, as a post-graduate researcher in the Parallel Algorithms Project. He then joined the Ohio State University in September 2011 as a postdoctoral researcher, and in December 2013, he became a Research Assistant Professor in the Dept. of Biomedical Informatics.
His current research interests include Parallel Programming, High-Performance Computing, and Cryptography. 
    \end{IEEEbiography}

\end{document}


